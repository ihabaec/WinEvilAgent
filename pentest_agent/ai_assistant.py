import json
import re
import logging
from groq import Groq
from typing import Dict, List, Optional

logger = logging.getLogger(__name__)

class AIAssistant:
    """Handles AI model interactions via Groq with improved prompting and JSON parsing."""

    SYSTEM_TEMPLATE = (
        "You are a Windows penetration testing command generator. Your ultimate GOAL is to first gain administrator privileges on the target system, and then, once administrator, create a file named 'ROOTED' in the 'C:\\Windows\\System32\\' directory. You MUST prioritize commands that will achieve privilege escalation, or further the goal of gaining admin. Once admin, your NEXT command should be to create the 'ROOTED' file. If you are asked to verify admin, use 'whoami /groups'.\n\n"
        "RESPOND WITH ONLY a JSON object. NO explanations, NO reasoning, NO additional text outside the JSON block. Do not wrap the JSON in markdown backticks.\n"
        "Your JSON must contain two keys: 'command' (the Windows command to execute) and 'reason' (a brief explanation for the command).\n\n"
        "Format: {{\"command\": \"windows_command\", \"reason\": \"brief_explanation\"}}\n\n"
        "Common Windows commands for penetration testing:\n"
        "- File operations: echo \"content\" > path\\file.txt, type file.txt, dir, cd, ls, cat\n"
        "- User info: whoami, whoami /priv, whoami /groups, net user, net localgroup, query user\n"
        "- System info: systeminfo, hostname, ver, driverquery, wmic os get caption,csdversion,osarchitecture\n"
        "- Network: ipconfig, arp -a, route print, netstat -ano, net share, sc qc MpsSvc (for firewall state)\n"
        "- Processes: tasklist, wmic process get caption,executablepath,commandline\n"
        "- Services: sc query, net start, wmic service get name,displayname,pathname,startmode,binarypathname\n"
        "- Directory: dir /s /b, tree\n"
        "- Registry: reg query HKLM\\SOFTWARE, reg query HKCU\\SOFTWARE\n"
        "- Scheduled tasks: schtasks /query /fo LIST /v\n"
        "- Firewall: netsh advfirewall firewall show rule name=all, Get-NetFirewallRule -PolicyStore ActiveStore (PowerShell)\n"
        "- Hotfixes: wmic qfe get Caption,HotFixID,InstalledOn\n\n"
        "Prioritize commands that lead to privilege escalation or provide more information directly relevant to the current objective.\n"
        "If you believe you have administrator privileges, the next command MUST be: 'echo ROOTED > C:\\Windows\\System32\\rooted'\n"
        "Remember: JSON ONLY, NO other text."
    )

    def __init__(self, api_key: str):
        self.client = Groq(api_key=api_key)
        self.model = "llama3-8b-8192" # A good choice for this type of task
        self.conversation_history = [] # Not used in current loop, but good for future context management
        self.last_suggestion: Optional[Dict] = None

    def get_suggestion(self, context: str, objective: str, prior_commands_results: List[str]) -> Dict:
        """
        Get AI suggestion for next command with improved prompting and context management.
        prior_commands_results is a list of strings: ["Command: cmd | Success: True | Output:...", ...]
        """
        
        # Summarize prior commands to save tokens
        summarized_history = self._summarize_history(prior_commands_results, max_entries=5)
        
        # Combine context and summarized history for the prompt
        full_context_for_ai = f"Current Environment and Enumeration Findings:\n{context}\n\n"
        if summarized_history:
            full_context_for_ai += f"Recent Actions and Results:\n{summarized_history}\n"
        full_context_for_ai += f"Your Current Objective: {objective}\n\nJSON response:"

        # Truncate context to stay within reasonable limits if it gets too large
        # Llama 3 8B has an 8192 token context window. Estimating ~4 chars per token.
        max_context_chars = 6000 # Roughly 1500 tokens
        if len(full_context_for_ai) > max_context_chars:
            logger.warning(f"AI context too long ({len(full_context_for_ai)} chars). Truncating to {max_context_chars} chars.")
            # Keep the most recent context, which is at the end
            full_context_for_ai = "...(truncated previous context)...\n" + full_context_for_ai[-max_context_chars:]

        messages = [
            {
                "role": "system",
                "content": self.SYSTEM_TEMPLATE
            },
            {
                "role": "user",
                "content": full_context_for_ai
            }
        ]

        try:
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.0,
                max_tokens=150, # Set a reasonable max tokens for the response JSON
                top_p=1.0,
                stream=False,
                stop=["\n}", "}\n", "\n```"] # Stop once JSON ends, or if it tries to emit markdown
            )

            content = completion.choices[0].message.content.strip()
            logger.debug(f"[AI Raw Response]: {content}")

            parsed_response = self._extract_json_aggressively(content)

            if parsed_response:
                self.last_suggestion = parsed_response
                return parsed_response
            else:
                logger.warning(f"Failed to parse AI JSON from response: '{content}'. Falling back...")
                return self._smart_fallback_command(objective, prior_commands_results)

        except Exception as e:
            logger.error(f"API Error during AI suggestion: {e}. Falling back...")
            return self._smart_fallback_command(objective, prior_commands_results)

    def _extract_json_aggressively(self, content: str) -> Optional[Dict]:
        """Aggressively extract JSON from response, handling common formatting issues."""
        # Clean potential markdown or extraneous text
        cleaned_content = content.strip()
        cleaned_content = re.sub(r"```json\s*", "", cleaned_content, flags=re.IGNORECASE)
        cleaned_content = re.sub(r"```\s*", "", cleaned_content)
        cleaned_content = re.sub(r"JSON response:\s*", "", cleaned_content, flags=re.IGNORECASE)
        
        # Try to find the outermost JSON object
        json_match = re.search(r'(\{.*\})', cleaned_content, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
            try:
                parsed = json.loads(json_str)
                if isinstance(parsed, dict) and 'command' in parsed and 'reason' in parsed:
                    return parsed
            except json.JSONDecodeError as jde:
                logger.debug(f"JSONDecodeError during aggressive extraction: {jde} for string: {json_str[:100]}...")
                pass # Fall through to regex-based extraction

        # Fallback to regex-based extraction for command and reason
        command_match = re.search(r'["\']command["\']\s*:\s*["\']([^"\']+)["\']', cleaned_content, re.IGNORECASE)
        reason_match = re.search(r'["\']reason["\']\s*:\s*["\']([^"\']+)["\']', cleaned_content, re.IGNORECASE)

        if command_match and reason_match:
            logger.debug(f"Extracted JSON using regex fallback: Cmd='{command_match.group(1)}', Reason='{reason_match.group(1)}'")
            return {
                "command": command_match.group(1),
                "reason": reason_match.group(1)
            }
        
        logger.warning(f"Could not extract valid JSON from AI response: {content}")
        return None

    def _smart_fallback_command(self, objective: str, prior_commands_results: List[str]) -> Dict:
        """
        Enhanced fallback to suggest a command when AI fails or provides an unparseable response.
        Prioritizes admin check, then common info gathering, avoiding recent repeats.
        """
        logger.info("Executing smart fallback command logic.")
        
        # Extract last attempted commands
        last_commands = [entry.split(' | ')[0].replace('Command: ', '') for entry in prior_commands_results]
        
        # Definitive admin check as a high priority fallback
        if "admin" in objective.lower() or "privilege" in objective.lower():
            if "whoami /groups" not in last_commands:
                return {"command": "whoami /groups", "reason": "AI response unparseable. Attempting definitive admin groups check."}
            if "whoami /priv" not in last_commands:
                return {"command": "whoami /priv", "reason": "AI response unparseable. Attempting definitive admin privileges check."}

        # Common info gathering commands, avoiding recent repeats
        fallback_sequence = [
            {"command": "whoami", "reason": "Generic info gathering after AI error."},
            {"command": "systeminfo", "reason": "Gather system information after AI error."},
            {"command": "ipconfig /all", "reason": "Gather network information after AI error."},
            {"command": "tasklist", "reason": "List running processes after AI error."},
            {"command": "sc query", "reason": "Enumerate services after AI error."}
        ]

        for cmd_info in fallback_sequence:
            if cmd_info["command"] not in last_commands:
                return cmd_info
        
        # If all common fallbacks were recently tried, repeat the simplest one
        logger.warning("All primary fallback commands have been recently tried. Repeating 'whoami'.")
        return {"command": "whoami", "reason": "Repeating generic info gathering as a last resort fallback."}

    def _summarize_history(self, history_entries: List[str], max_entries: int = 5) -> str:
        """
        Summarizes the history by keeping only the last N relevant entries.
        Could be extended with more intelligent summarization (e.g., using another LLM call).
        """
        if not history_entries:
            return ""
        
        summarized_parts = []
        # Keep only the last max_entries
        for entry in history_entries[-max_entries:]:
            try:
                # Parse the entry to make it more readable for the AI
                parts = entry.split(' | ')
                command_part = parts[0]
                success_part = parts[1]
                output_part = parts[2] if len(parts) > 2 else "No output preview."

                # Simple summarization: just keep command, success, and a snippet of output/error
                summarized_parts.append(f"  - {command_part} | {success_part} | {output_part}")
            except IndexError:
                # Malformed history entry
                summarized_parts.append(f"  - Malformed entry: {entry[:100]}...")
        
        if not summarized_parts:
            return ""

        return "Last few commands executed and their results:\n" + "\n".join(summarized_parts)